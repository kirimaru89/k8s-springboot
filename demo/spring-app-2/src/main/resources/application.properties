# --START PROMETHEUS
# Expose Prometheus endpoint
management.endpoints.web.exposure.include=health,info,prometheus
management.endpoint.health.show-details=always

# Enable all metrics
management.metrics.enable.all=true

# Enable JVM metrics
management.metrics.enable.jvm=true
# --END PROMETHEUS


spring.application.name=spring-app-2

# --START OPENTELEMETRY
# https://opentelemetry.io/docs/languages/java/configuration/
# https://opentelemetry.io/docs/zero-code/java/spring-boot-starter/out-of-the-box-instrumentation/

# send to otel collector via grpc, port 4317
otel.exporter.otlp.endpoint=http://my-opentelemetry-collector.monitoring.svc.cluster.local:4317
otel.exporter.otlp.protocol=grpc

otel.traces.sampler=parentbased_traceidratio
# development: 1.0, production: 0.1
otel.traces.sampler.arg=1.0

# --END OPENTELEMETRY

# must use jdbc:otel: prefix
# and use the driver-class-name for otel collector
spring.datasource.url=jdbc:postgresql://my-postgresql-1:5432/k8spostgres
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.username=postgres
spring.datasource.password=Q9mn6pUr0i
spring.jpa.properties.hibernate.jdbc.lob.non_contextual_creation=true


# kafka config: https://gist.github.com/geunho/77f3f9a112ea327457353aa407328771
# kafka broker address
spring.kafka.bootstrap-servers=my-kafka-broker.monitoring.svc.cluster.local:9092

# producer key serializer
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
# producer value serializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
# enable idempotence
spring.kafka.producer.properties.enable.idempotence=true
# acks
spring.kafka.producer.acks=all
# retries
spring.kafka.producer.retries=3
# max in flight requests per connection
spring.kafka.producer.max-in-flight-requests-per-connection=1
# transaction id prefix
spring.kafka.producer.transaction-id-prefix=tx-

spring.kafka.consumer.group-id=${spring.application.name}-group
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# # Add consumer error handling properties
spring.kafka.listener.ack-mode=MANUAL_IMMEDIATE
spring.kafka.consumer.enable-auto-commit=false
# only read committed messages to prevent duplicate messages processing
spring.kafka.consumer.properties.isolation.level=read_committed
spring.kafka.consumer.properties.max.poll.interval.ms=300000
spring.kafka.consumer.properties.max.poll.records=10

# # Kafka resilience configuration
spring.kafka.listener.missing-topics-fatal=false
kafka.listener.missing-topics-fatal=false

# # Make Kafka failure non-fatal for application startup
spring.kafka.consumer.properties.allow.auto.create.topics=true
spring.kafka.consumer.properties.request.timeout.ms=30000
spring.kafka.consumer.properties.default.api.timeout.ms=30000

spring.kafka.admin.auto-create=true
spring.kafka.admin.properties.num.partitions=1
spring.kafka.admin.properties.default.replication.factor=1

spring.data.redis.host=host.docker.internal
spring.data.redis.port=6379
# logging.level.io.lettuce=DEBUG